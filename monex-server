#!/usr/bin/env python3

from flask import Flask, jsonify, request, Response, make_response
from flask_cors import CORS, cross_origin
from datetime import datetime
import requests, sys, json, math, csv, io, argparse

app = Flask(__name__)

cors = CORS(app)
app.config['CORS_HEADERS'] = 'Content-Type'
experiences = []
targets = []
metrics = {}


def parse_conf(conf_file):
    with open(conf_file) as f:
        json_conf = json.load(f)
    for t in json_conf["targets"]:
        if t['server'] in [targ['server'] for targ in targets]:
            print("Config Error: {}".format(t['server']),file=sys.stderr)
            print("  Each server name must be unique",file=sys.stderr)
            sys.exit(1)
        targets.append(t)
    if "metrics" in json_conf.keys():
        for m in json_conf["metrics"]:
            metrics[m['name']] = m

def get_csv_from_influxdb(server, fields, start, end, measurement, database , graph_type):
    url = "http://{}/query".format(server)
    csv_file = io.StringIO()
    csv_writer = csv.writer(csv_file)
    if not fields:
        str_fields = '*'
    else:
        str_fields = '"' + '","'.join([f[0] for f in fields]) + '"'
    payload = {"epoch":"ns" ,"db":"{}".format(database),
               "q":'SELECT {} FROM "{}" WHERE time >= {}s and time <= {}s'
               .format(str_fields, measurement, start, end)}
    raw_data = requests.get(url, params=payload)
    print("Querying influxdb with:")
    print(raw_data.url)
    data = raw_data.json()
    columns = data['results'][0]['series'][0]['columns']
    columns[0] = graph_type
    csv_writer.writerow(columns)
    counter = 0
    t = start*(10**9)
    for val in data['results'][0]['series'][0]['values']:
        if graph_type == 'timestamp':
            time = val[0]/10**9
            line = [time] + val[1:]
        elif graph_type == 'duration':
            dur = (val[0] - t)/10**9
            line = [dur] + val[1:]
        else: #sample
            line = [counter] + val[1:]
            counter += 1
        csv_writer.writerow(line)
        output = make_response(csv_file.getvalue())
        output.headers["Content-type"] = "text/csv"
    return output


def get_csv_from_prometheus(server, query, start, end, step, labels, graph_type):
    url = "http://{}/api/v1/query_range".format(server)
    payload = {'query':query, 'start':start, 'end':end, 'step':step}
    raw_data = requests.get(url, params=payload)
    print("Querying prometheus with:")
    print(raw_data.url)
    data = raw_data.json()
    csv_file = io.StringIO()
    csv_writer = csv.writer(csv_file)
    if graph_type == 'timestamp':
        s = 0
        columns = ['timestamp']
    else: #duration
        s = start
        columns = ['duration']

    lines = []
    for t in data['data']['result'][0]['values']:
        lines.append([str(t[0] - s)])

    for serie in enumerate(data['data']['result']):
        if not labels:
            column_name = serie[0]
        elif type(labels) == str:
            column_name = serie[1]['metric'][serie_name]
        else:
            names = []
            for n in labels:
                names.append(serie[1]['metric'][n])
            column_name = '_'.join(names)
        columns.append(str(column_name))

        for value in enumerate(serie[1]['values']):
            lines[value[0]].append(str(value[1][1]))

    csv_writer.writerow(columns)

    for line in lines:
        csv_writer.writerow(line)

    output = make_response(csv_file.getvalue())
    output.headers["Content-type"] = "text/csv"
    return output


def get_current_time(local=None, utc=None):
    dec_loc_utc = math.ceil(datetime.now().timestamp() - datetime.utcnow().timestamp()) #SO UGLY
    if utc:
        utctime = int(utc)
        localtime = utctime + dec_loc_utc
    elif local:
        localtime =  int(local)
        utctime = localtime - dec_loc_utc
    else:
        localtime = datetime.now().timestamp()
        utctime = datetime.utcnow().timestamp()
    #We save local time and utc time...
    return int(localtime), int(utctime)


@app.route("/", methods=['POST','GET'])
def hello():
    return ""


@app.route("/search", methods=['POST'])
@cross_origin(max_age=600)
def search():
    data = {i[0] for i in experiences}
    return jsonify(list(data))


@app.route("/query", methods=['POST'])
def query():
    return jsonify({})


@app.route("/exp/<name>", methods=['POST'])
def start_xp(name):
    content = request.get_json()
    if content and 'localtime' in content.keys():
        time = get_current_time(local=content['localtime'])
    elif content and 'utctime' in content.keys():
        time = get_current_time(utc=content['utctime'])
    else:
        time = get_current_time()
    if name in [a[0] for a in experiences]:
        return "{} alredy started\n".format(name), 400
    else :
        experiences.append([name, 'start', time])
    return "Starting {}\n".format(name), 201


@app.route("/exp/<name>", methods=['PUT'])
def stop_xp(name):
    content = request.get_json()
    if content and 'localtime' in content.keys():
        time = get_current_time(local=content['localtime'])
    elif content and 'utctime' in content.keys():
        time = get_current_time(utc=content['utctime'])
    else:
        time = get_current_time()
    if name in [a[0] for a in experiences if a[1] == 'stop']:
        return "{} alredy stoped\n".format(name), 400
    elif name not in [a[0] for a in experiences if a[1] == 'start']:
        return "{} did not start\n".format(name), 400
    else:
        experiences.append([name, 'stop', time])
    return "Stopping {}\n".format(name), 201


@app.route("/exp/<name>", methods=['DELETE'])
def del_xp(name):
    if name not in [a[0] for a in experiences]:
        return "Could not find {}\n".format(name), 400
    else:
        res = []
        for exp in [todel for todel in experiences if todel[0] == name]:
            experiences.remove(exp)
    return "Deleting {}\n".format(name), 201


@app.route("/exp/", methods=['GET'])
def list_ann():
    s = ""
    for i in experiences:
        s+= "{} {} at {} (UTC)\n".format(i[0], i[1], datetime.fromtimestamp(i[2][1]))
    output = make_response(s)
    output.headers["Content-type"] = "text/plain"
    return output


@app.route("/exp/<name>", methods=['GET'])
def get_xp(name):
    content = request.get_json()
    if not content:
        return "Error: The server didn't recieve any JSON data\n", 400


    exp = name
    if 'metric' in content.keys():
        if content['metric'] in metrics:
            m = metrics[content['metric']]
            m.update(content)
            content = m
        else:
            return "Error: metric {} not found\n".format(content['metric']), 404

    server = content['server'] if 'server' in content.keys() else 'default'

    if exp not in [a[0] for a in experiences if a[1] == 'start']:
        return "{} did not start\n".format(exp), 400
    if exp not in [a[0] for a in experiences if a[1] == 'stop']:
        return "{} did not stop\n".format(exp), 400
    start = stop = None

    for i in experiences:
        if i[1] == 'stop' and i[0] == exp:
            end = i[2]
        elif i[1] == 'start' and i[0] == exp:
            start = i[2]

    for s in targets:
        if s['server'] == server:
            address = s["address"]
            server_type = s["type"]
            break
    else:
        return "{} not found\n".format(server), 404

    if server_type.lower() == 'prometheus':
        local_start = start[0]
        local_end = end[0]
        query = content['query']
        labels = content['labels'] if 'labels' in content.keys() else None
        step = content['step'] if 'step' in content.keys() else '1s'
        graph_type = content['type'] if 'type' in content.keys() else 'timestamp'
        if graph_type not in ['timestamp','duration']:
            return "Unknown type: {} \n Only timestamp and duration type supported for Prometheus\n".format(graph_type), 400
        return get_csv_from_prometheus(address, query, local_start, local_end, step, labels, graph_type)

    elif server_type.lower() == 'influxdb':
        utc_start = start[0]
        utc_end = end[0]
        fields = content['fields'] if 'fields' in content.keys() else None
        measurement = content['measurement']
        database = content['database'] if 'database' in content.keys() else 'monex'
        graph_type = content['type'] if 'type' in content.keys() else 'timestamp'
        if graph_type not in ['timestamp','duration','sample']:
            return "Unknown type: {} \n Only timestamp, sample and duration type supported for InfluxDB\n".format(graph_type), 400
        return get_csv_from_influxdb(address, fields, utc_start, utc_end, measurement, database, graph_type)
    else:
        return "Unsupported Target: {}\n".format(server_type), 404

@app.route("/annotations", methods=['POST'])
@cross_origin(max_age=600)
def annontations():
    content = request.get_json()
    query = content['annotation']['query']
    action, q = query.split(' ',1)
    q = q[1:-1].split(',')
    res = []
    for i in experiences:
        if i[1] == action and i[0] in q:
            res.append({'annotation': query, 'time': int(1000*i[2][0]),
                        'title': "{} {}".format(i[1],i[0]),
                        'tags': i[0]})
    return jsonify(res)


parser = argparse.ArgumentParser(prog="monex-server")
parser.add_argument("configuration", type= str,
    help="Configuration file")
parser.add_argument("-p", "--port", type= int, default=5000,
    help="port (default: 5000)")
args = parser.parse_args()
parse_conf(args.configuration)
app.run(host= '0.0.0.0', port=args.port)
